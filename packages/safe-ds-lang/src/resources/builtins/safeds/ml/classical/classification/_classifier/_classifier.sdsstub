package safeds.ml.classical.classification

from safeds.data.tabular.containers.Table import Table
from safeds.data.tabular.containers.TaggedTable import TaggedTable

/**
 * Abstract base class for all classifiers.
 */
class Classifier {
	/**
	 * Create a copy of this classifier and fit it with the given training data.
	 * 
	 * This classifier is not modified.
	 *
	 * @param trainingSet The training data containing the feature and target vectors.
	 *
	 * @result result1 The fitted classifier.
	 */
	@Pure
	fun fit(
		@PythonName("training_set") trainingSet: TaggedTable
	) -> result1: Classifier

	/**
	 * Predict a target vector using a dataset containing feature vectors. The model has to be trained first.
	 *
	 * @param dataset The dataset containing the feature vectors.
	 *
	 * @result result1 A dataset containing the given feature vectors and the predicted target vector.
	 */
	@Pure
	fun predict(
		dataset: Table
	) -> result1: TaggedTable

	/**
	 * Check if the classifier is fitted.
	 *
	 * @result result1 Whether the classifier is fitted.
	 */
	@Pure
	@PythonName("is_fitted")
	fun isFitted() -> result1: Boolean

	/**
	 * Compute the accuracy of the classifier on the given data.
	 *
	 * @param validationOrTestSet The validation or test set.
	 *
	 * @result result1 The calculated accuracy score, i.e. the percentage of equal data.
	 */
	@Pure
	fun accuracy(
		@PythonName("validation_or_test_set") validationOrTestSet: TaggedTable
	) -> result1: Float

	/**
	 * Compute the classifier's precision on the given data.
	 *
	 * @param validationOrTestSet The validation or test set.
	 * @param positiveClass The class to be considered positive. All other classes are considered negative.
	 *
	 * @result result1 The calculated precision score, i.e. the ratio of correctly predicted positives to all predicted positives.
	 * Return 1 if no positive predictions are made.
	 */
	@Pure
	fun precision(
		@PythonName("validation_or_test_set") validationOrTestSet: TaggedTable,
		@PythonName("positive_class") positiveClass: Any
	) -> result1: Float

	/**
	 * Compute the classifier's recall on the given data.
	 *
	 * @param validationOrTestSet The validation or test set.
	 * @param positiveClass The class to be considered positive. All other classes are considered negative.
	 *
	 * @result result1 The calculated recall score, i.e. the ratio of correctly predicted positives to all expected positives.
	 * Return 1 if there are no positive expectations.
	 */
	@Pure
	fun recall(
		@PythonName("validation_or_test_set") validationOrTestSet: TaggedTable,
		@PythonName("positive_class") positiveClass: Any
	) -> result1: Float

	/**
	 * Compute the classifier's $F_1$-score on the given data.
	 *
	 * @param validationOrTestSet The validation or test set.
	 * @param positiveClass The class to be considered positive. All other classes are considered negative.
	 *
	 * @result result1 The calculated $F_1$-score, i.e. the harmonic mean between precision and recall.
	 * Return 1 if there are no positive expectations and predictions.
	 */
	@Pure
	@PythonName("f1_score")
	fun f1Score(
		@PythonName("validation_or_test_set") validationOrTestSet: TaggedTable,
		@PythonName("positive_class") positiveClass: Any
	) -> result1: Float
}
