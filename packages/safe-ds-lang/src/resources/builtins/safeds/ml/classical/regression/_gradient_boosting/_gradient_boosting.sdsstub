package safeds.ml.classical.regression

from safeds.data.tabular.containers.Table import Table
from safeds.data.tabular.containers.TaggedTable import TaggedTable
from safeds.ml.classical.regression.Regressor import Regressor

/**
 * Gradient boosting regression.
 *
 * @param numberOfTrees The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large
 * number usually results in better performance.
 * @param learningRate The larger the value, the more the model is influenced by each additional tree. If the learning rate is too
 * low, the model might underfit. If the learning rate is too high, the model might overfit.
 */
class GradientBoosting(
	@PythonName("number_of_trees") numberOfTrees: Int = 100,
	@PythonName("learning_rate") learningRate: Float = 0.1
) sub Regressor {
	/**
	 * Get the number of trees (estimators) in the ensemble.
	 */
	@PythonName("number_of_trees") attr numberOfTrees: Int
	/**
	 * Get the learning rate.
	 */
	@PythonName("learning_rate") attr learningRate: Float

	/**
	 * Create a copy of this regressor and fit it with the given training data.
	 * 
	 * This regressor is not modified.
	 *
	 * @param trainingSet The training data containing the feature and target vectors.
	 *
	 * @result result1 The fitted regressor.
	 */
	@Pure
	fun fit(
		@PythonName("training_set") trainingSet: TaggedTable
	) -> result1: GradientBoosting

	/**
	 * Predict a target vector using a dataset containing feature vectors. The model has to be trained first.
	 *
	 * @param dataset The dataset containing the feature vectors.
	 *
	 * @result result1 A dataset containing the given feature vectors and the predicted target vector.
	 */
	@Pure
	fun predict(
		dataset: Table
	) -> result1: TaggedTable

	/**
	 * Check if the regressor is fitted.
	 *
	 * @result result1 Whether the regressor is fitted.
	 */
	@Pure
	@PythonName("is_fitted")
	fun isFitted() -> result1: Boolean
}
