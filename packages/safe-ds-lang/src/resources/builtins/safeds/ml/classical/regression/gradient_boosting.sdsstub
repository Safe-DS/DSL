package safeds.ml.classical.regression

from safeds.data.tabular.containers import Table, TaggedTable
from safeds.ml.classical.regression import Regressor

/**
 * Gradient boosting regression.
 *
 * @param numberOfTrees The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large
 * number usually results in better performance.
 * @param learningRate The larger the value, the more the model is influenced by each additional tree. If the learning rate is too
 * low, the model might underfit. If the learning rate is too high, the model might overfit.
 *
 * @example
 * pipeline example {
 *     val training = Table.fromCsvFile("training.csv").tagColumns("target");
 *     val test = Table.fromCsvFile("test.csv").tagColumns("target");
 *     val regressor = GradientBoostingRegressor(numberOfTrees = 50).fit(training);
 *     val meanSquaredError = regressor.meanSquaredError(test);
 * }
 */
class GradientBoostingRegressor(
    @PythonName("number_of_trees") const numberOfTrees: Int = 100,
    @PythonName("learning_rate") const learningRate: Float = 0.1
) sub Regressor where {
    numberOfTrees >= 1,
    learningRate > 0.0
} {
    /**
     * Get the number of trees (estimators) in the ensemble.
     */
    @PythonName("number_of_trees") attr numberOfTrees: Int
    /**
     * Get the learning rate.
     */
    @PythonName("learning_rate") attr learningRate: Float

    /**
     * Create a copy of this regressor and fit it with the given training data.
     *
     * This regressor is not modified.
     *
     * @param trainingSet The training data containing the feature and target vectors.
     *
     * @result fittedRegressor The fitted regressor.
     */
    @Pure
    fun fit(
        @PythonName("training_set") trainingSet: TaggedTable
    ) -> fittedRegressor: GradientBoostingRegressor
}
